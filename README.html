<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="introduction">Introduction</h1>
<p>This project is my (Matthias Schinacher) solution to a homework assignment for Udacity's Deep Reinforcement Learning Nano Degree.<br />
It contains mainly a python implementation of the Q- learning algorithm with replay-memory, using a neural network implemented with pytorch as the Q-function approximation.</p>
<h1 id="project-details">Project Details</h1>
<p>The environment ist very similar to/ a variant of the &quot;Bananas Collector&quot; environment from Unity; <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector">Unity ML-Agents GitHub</a>.</p>
<p>In the environment an agent navigates a large square worlds. Within this world there are yellow and purple bananas (and they also pop into existence). The agent has 4 possible actions (forward, backward, left right) and needs to increase his score by collecting yellow bananas (+1) while avoiding to hit puple ones (-1).</p>
<p>The environment can be accessed via python/ has a python interface. The state space is a vector of 37 numeric values (that represent the agents velocity and ray- based perceptions).</p>
<p>The defined goal of the homework/ project is/was to achieve a &quot;sustained&quot; score of at least 13 per episode. That means, that the algorithm/ the model should be able to average above score 13 for &quot;the last 100 episodes&quot; over a number of episodes.</p>
<h1 id="dependencies">Dependencies</h1>
<p>The actual &quot;program&quot; (agent) is a python script that can be run from the command line. To be able to run it, python 3.6 must be installed.</p>
<h2 id="python-packages">Python packages</h2>
<p>The following packages/ libraries are needed</p>
<ul>
<li>tensorflow, version 1.7.1</li>
<li>numpy, at least version 1.11.0</li>
<li>torch, version 0.4.0 (pytorch)</li>
</ul>
<h2 id="other-dependecies">Other dependecies</h2>
<p>A minimal install of OpenAI gym is required, as well as the classic control environment group and the box2d environment group; instructions how to install this <a href="https://github.com/openai/gym">can be found here</a>.</p>
<p>Additionally one needs the &quot;Banana&quot; environment from udacity, which was created for the course. This can be downloaded <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip">for Linux</a> (and other operating systems).</p>
<h1 id="running-the-script-program-agent">Running the script/ program/ agent</h1>
<p>To run the script from the command line (Linux), the dependencies mentioned must be installed and the contents of the &quot;Banana_Linux.zip&quot; need to be unzipped in the same directory, where the actual script &quot;ms_drlndnav_pr.py&quot; resides, so that we have a subdirectory &quot;Banana_Linux&quot;.</p>
<pre><code>python ms_drlndnav_pr.py command-file.ini</code></pre>
<p>will start the agent as per the parameters in the file &quot;command-file.ini&quot;. Depending on the contents of the command-file, the agent will try to solve the environment and train a neural network to approximate a state-action function; or the script will load a predefined NN- model from a file and only play the banana- environment as a game with a fixed strategy. For more details see also the project- report.</p>
<h2 id="parameters">Parameters</h2>
<p>The format of a command-file is like that of the standard python &quot;configparser&quot; lib, which is similar to the known Windows INI format (see examples from the command ZIP- archive).</p>
<ul>
<li>global
<ul>
<li>runlog: name of the logfile to use</li>
</ul></li>
<li>mode
<ul>
<li>train: whether we're in training mode</li>
<li>show: flag, whether to show the game in &quot;human time&quot;</li>
</ul></li>
<li>rand
<ul>
<li>seed: seed for random number generation</li>
</ul></li>
<li>model
<ul>
<li>h1: first size- parameter for the NN- model</li>
<li>h2: second size- parameter for the NN- model</li>
<li>load_file: load model from this file</li>
<li>load_transitions_file: load replay-memory transitions from this file</li>
<li>save_file: save final Q- model to this file</li>
<li>save_best_file: save Q- model from episode with highest score to this file</li>
<li>save_transitions_file: save replay-memory transitions to this file</li>
</ul></li>
<li>hyperparameters
<ul>
<li>episodes: number of episodes to run</li>
<li>warmup_episodes: epiosodes to run with pure random sampling</li>
<li>epsilon_episodes: number of episodes over which to descrease epsilon</li>
<li>epsilon_start: start- value for epsilon</li>
<li>epsilon_end: final value for epsilon</li>
<li>replay_buffersize: size of the replay memory</li>
<li>replay_batchsize: number of transitions to sample per optimizing step</li>
<li>replay_steps: game-steps between each optimizing step</li>
<li>prio_replay: flag, whether to use priority replay</li>
<li>q_reset_steps: reset the parameters of the &quot;fixed&quot; Q- model after this many steps</li>
<li>gamma: gamma (Q- learning parameter)</li>
<li>learning_rate: the learning rate</li>
</ul></li>
</ul>
<h3 id="example-command-file-contents">Example command-file contents</h3>
<pre><code>[global]  
runlog = test4.log
 
[mode]
train = 1

[rand]
seed = 4719

[model]
h1 = 10
h2 = 10
save_file = test4.model
save_best_file = test4.best.model
save_transitions_file = test4.transitions

[hyperparameters]
episodes           = 1000
warmup_episodes    = 10
epsilon_episodes   = 200
epsilon_start      = 0.99
epsilon_end        = 0.01
replay_buffersize  = 30000
replay_batchsize   = 300
replay_steps       = 1
prio_replay        = False
q_reset_steps      = 30
gamma              = 0.99
learning_rate      = 0.001</code></pre>
<h2 id="output">Output</h2>
<h3 id="logfile">Logfile</h3>
<p>The main output is a log file which contains various information as within #- style comment lines and the time-series data of - Episode- number - Score (at episode end) - Average Score of the last 100 episodes (the very episode score and the 99 episodes before it) - The number of steps in this episode - The epsilon used for the episode</p>
<p>Example:</p>
<pre><code>...
# Episode Score average(last-100-Scores) Steps Epsilon
1 1.0 0.0 300 0.99
...
16 0.0 0.0 300 0.9607462686567165
17 0.0 0.0 300 0.9558706467661693
18 -3.0 0.0 300 0.950995024875622
19 1.0 0.0 300 0.9461194029850748
20 0.0 0.0 300 0.9412437810945276
21 -1.0 0.0 300 0.9363681592039803
22 -1.0 0.0 300 0.9314925373134331
23 0.0 0.0 300 0.9266169154228858
...</code></pre>
<h3 id="model-file">Model file</h3>
<p>The script allowes for the saving/loading of the NN- model in the binary format as used by pytorch's functions &quot;torch.load(..)&quot; and &quot;torch.save(..)&quot;.</p>
<h3 id="transitions-replay-memory">Transitions/ replay-memory</h3>
<p>The script allowes for the saving/loading of the transition- replay-memory (replay-buffer) using the binary pickle- format as from the python pickle- module.</p>
<h1 id="the-solution">The solution</h1>
<p>The solution to the project task is, apart from the python script, contained within the command file &quot;test4.ini&quot;. To recreate the model and plot data, extract &quot;test4.ini&quot; from the &quot;command-file&quot;- ZIP archive and run:</p>
<pre><code>python ms_drlndnav_pr.py test4.ini</code></pre>
<p>After that the file &quot;test4.log&quot; should contain the time-series output of the experiment and the files &quot;test4.model&quot;, &quot;test4.best.model&quot; should contain the model and &quot;test4.transitions&quot; the final replay-memory.</p>
<div class="figure">
<img src="test4.png" alt="My solution" />
<p class="caption">My solution</p>
</div>
<h1 id="misc">Misc</h1>
<h2 id="model-output">Model output</h2>
<p>With the simple python script &quot;print_model.py&quot; it's possible to output the NN- model parameters from a model- file that was written by a run of the main script. Example:</p>
<pre><code>python print_model.py test4.model</code></pre>
<h2 id="the-best-model">The &quot;best&quot; model</h2>
<p>The program/ script can output 2 different models at the end of a run. The &quot;best&quot; model, that is the model at the end of the highest scoring episode, and the &quot;normal&quot; model at the end of the script run, the final model corresponding with the potentially also saved replay-memory (pickle file).</p>
<h2 id="zip--archives">ZIP- archives</h2>
<h3 id="inis.zip">INIS.zip</h3>
<p>List of sample command-files, that I used for my experiments.</p>
<h3 id="logs.zip">LOGS.zip</h3>
<p>Logs from the script runs with the sample command-files.</p>
<h3 id="models.zip">MODELS.zip</h3>
<p>Models and transitions/replay-buffer (binary format) from the script runs with the sample command-files.</p>
<h3 id="pngs.zip">PNGS.zip</h3>
<p>Pictures created with gnuplot from the log-files and used for the project report.</p>
<h2 id="see-also">See also</h2>
<ul>
<li>report.pdf: the project report, contains additional information</li>
</ul>
</body>
</html>
